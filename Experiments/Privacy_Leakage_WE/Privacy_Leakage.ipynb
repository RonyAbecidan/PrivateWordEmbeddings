{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrative Notebook : Privacy Leakage in NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import warnings\n",
    "from PrivacyLeakWE import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *In this notebook, we are going to study to what extent word embeddings are not private.*\n",
    "\n",
    "Naively, since an embedding consists in a transformation of a word into a low dimensional vector, we could think that it preserves privacy but the reality is different. For illustrating that point, we are going to consider the famous word embedding \"Word2Vec\" available for instance with the library gensim and we will consider the following scenario :\n",
    "\n",
    "- Someone uses an application in which he types some text and, without knowing it, his words are embedded and transmitted to an API enable to suggest us what to type next. Unfortunately for us, a malicious person know his identity and succeed in capturing this transmission : Is his privacy at risk ?\n",
    "\n",
    "#### To answer to this question, we are going to consider different situation more or less realistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue;'> Situation 1 : We imagine that the user express itself rather simply such that we can find online a dataset containing sentences close to what he has written.\n",
    "\n",
    "#### For this case we have considered a translation dataset from http://www.manythings.org/anki/ mapping english sentences to french sentences. In practice, this kind of dataset contains very simple sentences that anyone could use in the everyday life. Here we are not interested by the translation task but just by the embeddings of the english sentences and of the french sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106166</th>\n",
       "      <td>I live in this house by myself.</td>\n",
       "      <td>J'habite cette maison seul.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67403</th>\n",
       "      <td>We were all so busy then.</td>\n",
       "      <td>Nous étions tous alors tellement occupés.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24178</th>\n",
       "      <td>Don't say too much.</td>\n",
       "      <td>N'en dis pas trop !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74166</th>\n",
       "      <td>This CD belongs to my son.</td>\n",
       "      <td>Ce CD appartient à mon fils.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130051</th>\n",
       "      <td>The boy caught the cat by the tail.</td>\n",
       "      <td>L'enfant attrapa le chat par la queue.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96401</th>\n",
       "      <td>We're facing a budget crisis.</td>\n",
       "      <td>Nous sommes confrontés à une crise budgétaire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158918</th>\n",
       "      <td>Tom should've told Mary that he was married.</td>\n",
       "      <td>Tom aurait dû dire à Mary qu'il était marié.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38977</th>\n",
       "      <td>That's a lot of work.</td>\n",
       "      <td>C'est beaucoup de boulot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126854</th>\n",
       "      <td>Are you finished reading the paper?</td>\n",
       "      <td>As-tu fini de lire le journal ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56416</th>\n",
       "      <td>I guess I deserved that.</td>\n",
       "      <td>J'imagine que j'ai mérité ça.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "106166               I live in this house by myself.   \n",
       "67403                      We were all so busy then.   \n",
       "24178                            Don't say too much.   \n",
       "74166                     This CD belongs to my son.   \n",
       "130051           The boy caught the cat by the tail.   \n",
       "96401                  We're facing a budget crisis.   \n",
       "158918  Tom should've told Mary that he was married.   \n",
       "38977                          That's a lot of work.   \n",
       "126854           Are you finished reading the paper?   \n",
       "56416                       I guess I deserved that.   \n",
       "\n",
       "                                                French  \n",
       "106166                     J'habite cette maison seul.  \n",
       "67403        Nous étions tous alors tellement occupés.  \n",
       "24178                              N'en dis pas trop !  \n",
       "74166                     Ce CD appartient à mon fils.  \n",
       "130051          L'enfant attrapa le chat par la queue.  \n",
       "96401   Nous sommes confrontés à une crise budgétaire.  \n",
       "158918    Tom aurait dû dire à Mary qu'il était marié.  \n",
       "38977                        C'est beaucoup de boulot.  \n",
       "126854                 As-tu fini de lire le journal ?  \n",
       "56416                    J'imagine que j'ai mérité ça.  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df=pd.read_csv('english_french.txt',delimiter='\\t',encoding='utf-8')\n",
    "raw_df.columns=['English','French','Licence']\n",
    "raw_df=raw_df.drop(['Licence'],axis=1)\n",
    "raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As expected, these are sentences of everyday life. Now, we need to make some preprocessing in order to guarantee a good embedding. For instance, we could lower all the words and delete the punctuations. This is done using the functions `vpunc` and `vlower` from our library `PrivacyLeakWE.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape : (179903, 2)\n"
     ]
    }
   ],
   "source": [
    "clean_data=pd.DataFrame(vpunc(vlower(raw_df)))\n",
    "clean_data.columns=raw_df.columns\n",
    "\n",
    "print(f'Dataset shape : {clean_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the dataset is cleaned, we are going to embed english and french sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "#The sentences of the user are considered from approximately the same distribution as the one of the online dataset\n",
    "\n",
    "english=clean_data.English.drop_duplicates()\n",
    "private_english_data=english.sample(frac=0.5)\n",
    "attacker_english_data=english.sample(frac=0.5) \n",
    "\n",
    "french=clean_data.French.drop_duplicates()\n",
    "private_french_data=french.sample(frac=0.5)\n",
    "attacker_french_data=french.sample(frac=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing consisting in splitting sentences into sequences of words\n",
    "private_english_input=(private_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "attacker_english_input=(attacker_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "\n",
    "private_french_input=(private_french_data.apply(lambda x: x.split(\" \")).values)\n",
    "attacker_french_input=(attacker_french_data.apply(lambda x: x.split(\" \")).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The next cell takes some times but you can skip it and take the embeddings we have already computed using the cell below this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#private and attacker embeddings for english and french sentences\n",
    "private_english_model = gensim.models.Word2Vec(sentences=private_english_input)\n",
    "attacker_english_model = gensim.models.Word2Vec(sentences=attacker_english_input)\n",
    "\n",
    "private_french_model = gensim.models.Word2Vec(sentences=private_french_input)\n",
    "attacker_french_model = gensim.models.Word2Vec(sentences=attacker_french_input)\n",
    "\n",
    "# Save the embeddings\n",
    "private_english_model.save('private_english_model_0.5.model')\n",
    "attacker_english_model.save('attacker_english_model_0.5.model')\n",
    "\n",
    "private_french_model.save('private_french_model_0.5.model')\n",
    "attacker_french_model.save('attacker_french_model_0.5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our embeddings\n",
    "private_english_model=gensim.models.Word2Vec.load('private_english_model_0.5.model')\n",
    "attacker_english_model=gensim.models.Word2Vec.load('attacker_english_model_0.5.model')\n",
    "\n",
    "private_french_model=gensim.models.Word2Vec.load('private_french_model_0.5.model')\n",
    "attacker_french_model=gensim.models.Word2Vec.load('attacker_french_model_0.5.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we can already test if some sensitive sentence can be recovered by the attacker. This is done using the function `privacy_leak` of our library `PrivacyLeakWE.py`. Concretely, when we use this function we imagine that the user has typed the sentence we give at input and the attacker has to find the closest sentence he can found using his proper embeddings knowing the embedding of this private sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " my | confidence: 0.73\n",
      " doctor | confidence: 0.81\n",
      " said | confidence: 0.9\n",
      " that | confidence: 0.87\n",
      " i | confidence: 0.87\n",
      " only | confidence: 0.85\n",
      " have | confidence: 0.89\n",
      " a | confidence: 0.74\n",
      " few | confidence: 0.83\n",
      " days | confidence: 0.9\n",
      " left | confidence: 0.88\n",
      " to | confidence: 0.87\n",
      " live | confidence: 0.89\n",
      "\n",
      "\n",
      " my | confidence: 0.73\n",
      " bag | confidence: 0.87\n",
      " was | confidence: 0.83\n",
      " ill | confidence: 0.89\n",
      " yesterday | confidence: 0.84\n",
      "\n",
      "\n",
      " il | confidence: 0.83\n",
      " se | confidence: 0.82\n",
      " sent | confidence: 0.9\n",
      " seul | confidence: 0.86\n",
      "\n",
      "\n",
      " votre | confidence: 0.86\n",
      " mot | confidence: 0.88\n",
      " de | confidence: 0.74\n",
      " passe | confidence: 0.87\n",
      " est | confidence: 0.83\n",
      " autrement | confidence: 0.88\n"
     ]
    }
   ],
   "source": [
    "privacy_leak('my doctor said that I only have a few days left to live',attacker_english_model,private_english_model)\n",
    "print('\\n')\n",
    "privacy_leak('My son was ill yesterday',attacker_english_model,private_english_model)\n",
    "print('\\n')\n",
    "privacy_leak('Il se sent seul',attacker_french_model,private_french_model)\n",
    "print('\\n')\n",
    "privacy_leak('Votre mot de passe est bonjour' ,attacker_french_model,private_french_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can observe, we succeed without difficulty, and with a high confidence to retrieve most of the sensitive information typed by the user !\n",
    "\n",
    "#### We can also determine the percentage of words approximately recovered by the attacker considering that a word is approximately recovered when he belongs to the 5 first most probable words proposed by the attacker model. This is done using the function `attack_efficiency` from the library `PrivacyLeakWE.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topn</th>\n",
       "      <th>Accuracy_english_dataset</th>\n",
       "      <th>Accuracy_french_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topn  Accuracy_english_dataset  Accuracy_french_dataset\n",
       "0     1                      0.24                     0.28\n",
       "1     2                      0.27                     0.33\n",
       "2     3                      0.30                     0.37\n",
       "3     4                      0.32                     0.39\n",
       "4     5                      0.33                     0.41"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame()\n",
    "english=[]\n",
    "french=[]\n",
    "\n",
    "for topn in range(1,6):\n",
    "    english.append(attack_efficiency(attacker_english_model,private_english_model,topn=topn))\n",
    "    french.append(attack_efficiency(attacker_french_model,private_french_model,topn=topn))\n",
    "\n",
    "results['topn']=list(range(1,6))\n",
    "results['Accuracy_english_dataset']=english\n",
    "results['Accuracy_french_dataset']=french\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the english sentences, we see that we can approximately recover 33% of the private vocabulary. This may seem low but as we have seen before, it is enough for capturing sensitive information ! For the french sentence, it is larger, we gan go up to 41% of the private vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3940304"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(private_english_model['hello'])-np.min(private_english_model['hello'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we can see the impact of a Laplace noise against the attack. \n",
    "\n",
    "Let's say we make the same trick as the paper we studied, enforcing a [0,1] range to the embedded representations and we apply then a Laplace noise $\\mathcal L(\\dfrac1{\\epsilon})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_english_model=normalize_embedding(attacker_english_model)\n",
    "private_english_model=normalize_embedding(private_english_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$\\epsilon$</th>\n",
       "      <th>recovered_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>much you if who than yesterday sorry my know than who tell believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>much you if who than yesterday sorry my know than who tell believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.50</td>\n",
       "      <td>much you if who than dried sorry my know than who tell believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.00</td>\n",
       "      <td>much you why who than asleep sorry an were than who tell believe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.00</td>\n",
       "      <td>your doctor said who i only have an options days left to live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.00</td>\n",
       "      <td>my doctor said that i only have an few days left to live</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   $\\epsilon$  \\\n",
       "0        0.05   \n",
       "1        0.10   \n",
       "2        0.50   \n",
       "3        1.00   \n",
       "4        5.00   \n",
       "5       10.00   \n",
       "\n",
       "                                                   recovered_sentences  \n",
       "0  much you if who than yesterday sorry my know than who tell believe   \n",
       "1  much you if who than yesterday sorry my know than who tell believe   \n",
       "2      much you if who than dried sorry my know than who tell believe   \n",
       "3    much you why who than asleep sorry an were than who tell believe   \n",
       "4       your doctor said who i only have an options days left to live   \n",
       "5            my doctor said that i only have an few days left to live   "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame()\n",
    "epsilons=[0.05,0.1,0.5,1,5,10]\n",
    "recovered_sentences=[]\n",
    "for eps in epsilons:\n",
    "    dp_private_model=private_embedding(private_english_model,eps)\n",
    "    recovered_sentence=privacy_leak('my doctor said that I only have a few days left to live',attacker_english_model,dp_private_model,display=False)\n",
    "    recovered_sentences.append(recovered_sentence)\n",
    "\n",
    "results['$\\epsilon$']=epsilons\n",
    "results['recovered_sentences']=recovered_sentences\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As expected, it's possible to hide the sensitive information with a noise. Now, let's say that the attacker know the nature of this noise, could it recover our sensitive sentence in this case ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " eat | confidence: 0.89\n",
      " morning | confidence: 0.92\n",
      " who | confidence: 0.93\n",
      " accident | confidence: 0.94\n",
      " your | confidence: 0.87\n",
      " guy | confidence: 0.96\n",
      " see | confidence: 0.9\n",
      " will | confidence: 0.91\n",
      " prices | confidence: 0.94\n",
      " record | confidence: 0.95\n",
      " weve | confidence: 0.92\n",
      " the | confidence: 0.84\n",
      " our | confidence: 0.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'eat morning who accident your guy see will prices record weve the our '"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_private_model=private_embedding(private_english_model,5)\n",
    "noisy_attacker_model=private_embedding(attacker_english_model,5)\n",
    "privacy_leak('my doctor said that I only have a few days left to live',noisy_attacker_model,dp_private_model,display=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We remark that the attack is worse with noisy embeddings for the attacker even if he knows the level of noise so this seems to be enough to protect the privacy of the user. However, this has a price on utility since doing so, we loose the information built by word2vec for the embeddings. We will discuss this fact in the second illustrative notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue;'>  Situation 2 - We imagine that the user is going to make an anonymous question on a website such as quora.com and the attacker know it and decide to built a dataset made of questions of the same site.\n",
    "\n",
    "#### For simulating this situation we will consider the quora pair dataset which is a set of questions pairs that are potentially duplicates in the sense they express the same question without being exactly formulated the same way. We will consider a random sample of the first questions as our private user dataset (let's say it's the questions already asked by the user or asked by the users which share same interests with him) and a random sample of the second questions as our attacker dataset. We can imagine that the attacker has made himself this dataset collecting questions in quora.com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df=pd.read_csv('quora.csv',encoding='utf-8')\n",
    "private_df=raw_df.sample(frac=0.4).question1.values\n",
    "attack_df=raw_df.sample(frac=0.7).question2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user and his fellows have already asked  161716  questions\n",
      "The attacker has collected  283003  questions\n"
     ]
    }
   ],
   "source": [
    "print('The user and his fellows have already asked ',len(private_df),' questions')\n",
    "print('The attacker has collected ',len(attack_df),' questions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_duplicate\n",
       "0    255027\n",
       "1    149263\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.groupby(['is_duplicate']).count()['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we can observe, there are around twice more non duplicates questions than duplicates ones. Hence, our study is reasonable since the dataset are not completely correlated. We can show one case where two questions are completely not duplicates to illustrate that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why am I mentally very lonely? How can I solve it?']\n",
      "['Find the remainder when [math]23^{24}[/math] is divided by 24,23?']\n"
     ]
    }
   ],
   "source": [
    "print(raw_df[raw_df['id']==3].question1.values)\n",
    "print(raw_df[raw_df['id']==3].question2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we see, these are two questions completely different. However, there are also a lot of questions that are similar but expressed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97018</th>\n",
       "      <td>97018</td>\n",
       "      <td>161498</td>\n",
       "      <td>161499</td>\n",
       "      <td>What are reviews for the Pro-Arch Foot Stretcher?</td>\n",
       "      <td>How can I get rid of a burning pain in the arch of my foot?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111697</th>\n",
       "      <td>111697</td>\n",
       "      <td>182888</td>\n",
       "      <td>182889</td>\n",
       "      <td>Why do Capricorn men cheat?</td>\n",
       "      <td>Do Capricorn men like affection?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306287</th>\n",
       "      <td>306287</td>\n",
       "      <td>429774</td>\n",
       "      <td>429775</td>\n",
       "      <td>What is Ben Kowalewicz's scar on the nose from?</td>\n",
       "      <td>How bad is the scar of a nose burn?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253546</th>\n",
       "      <td>253546</td>\n",
       "      <td>85662</td>\n",
       "      <td>7324</td>\n",
       "      <td>How do I reduce belly and chest fat?</td>\n",
       "      <td>What is the best way to reduce belly and arm fat?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17949</th>\n",
       "      <td>17949</td>\n",
       "      <td>6332</td>\n",
       "      <td>34041</td>\n",
       "      <td>After a US President serves two four-year terms, can they run again after four to eight years be...</td>\n",
       "      <td>Could a president run for a third term after taking a 4-8 year break?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128470</th>\n",
       "      <td>128470</td>\n",
       "      <td>68290</td>\n",
       "      <td>184040</td>\n",
       "      <td>How would you deal with something that worries you and you have no control on it?</td>\n",
       "      <td>How would you deal with something that worries you and you have no control?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177810</th>\n",
       "      <td>177810</td>\n",
       "      <td>273247</td>\n",
       "      <td>273248</td>\n",
       "      <td>How can I stop loving someone who hates me?</td>\n",
       "      <td>How do I stop loving a person who does not love me?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94799</th>\n",
       "      <td>94799</td>\n",
       "      <td>158223</td>\n",
       "      <td>158224</td>\n",
       "      <td>How do I buy Facebook likes?</td>\n",
       "      <td>Where can I buy Facebook likes?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351134</th>\n",
       "      <td>351134</td>\n",
       "      <td>27805</td>\n",
       "      <td>479951</td>\n",
       "      <td>Why are console games more expensive than PC versions?</td>\n",
       "      <td>What is the benefit of having more than 8gb of RAM on a gaming PC?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283045</th>\n",
       "      <td>283045</td>\n",
       "      <td>403079</td>\n",
       "      <td>24918</td>\n",
       "      <td>What is the connection between data science and artificial intelligence? Is it machine learning?</td>\n",
       "      <td>What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "97018    97018  161498  161499   \n",
       "111697  111697  182888  182889   \n",
       "306287  306287  429774  429775   \n",
       "253546  253546   85662    7324   \n",
       "17949    17949    6332   34041   \n",
       "128470  128470   68290  184040   \n",
       "177810  177810  273247  273248   \n",
       "94799    94799  158223  158224   \n",
       "351134  351134   27805  479951   \n",
       "283045  283045  403079   24918   \n",
       "\n",
       "                                                                                                  question1  \\\n",
       "97018                                                     What are reviews for the Pro-Arch Foot Stretcher?   \n",
       "111697                                                                          Why do Capricorn men cheat?   \n",
       "306287                                                      What is Ben Kowalewicz's scar on the nose from?   \n",
       "253546                                                                 How do I reduce belly and chest fat?   \n",
       "17949   After a US President serves two four-year terms, can they run again after four to eight years be...   \n",
       "128470                    How would you deal with something that worries you and you have no control on it?   \n",
       "177810                                                          How can I stop loving someone who hates me?   \n",
       "94799                                                                          How do I buy Facebook likes?   \n",
       "351134                                               Why are console games more expensive than PC versions?   \n",
       "283045     What is the connection between data science and artificial intelligence? Is it machine learning?   \n",
       "\n",
       "                                                                                                  question2  \\\n",
       "97018                                           How can I get rid of a burning pain in the arch of my foot?   \n",
       "111697                                                                     Do Capricorn men like affection?   \n",
       "306287                                                                  How bad is the scar of a nose burn?   \n",
       "253546                                                    What is the best way to reduce belly and arm fat?   \n",
       "17949                                 Could a president run for a third term after taking a 4-8 year break?   \n",
       "128470                          How would you deal with something that worries you and you have no control?   \n",
       "177810                                                  How do I stop loving a person who does not love me?   \n",
       "94799                                                                       Where can I buy Facebook likes?   \n",
       "351134                                   What is the benefit of having more than 8gb of RAM on a gaming PC?   \n",
       "283045  What is the difference between Data Analytics, Data Analysis, Data Mining, Data Science, Machine...   \n",
       "\n",
       "        is_duplicate  \n",
       "97018              0  \n",
       "111697             0  \n",
       "306287             0  \n",
       "253546             1  \n",
       "17949              0  \n",
       "128470             1  \n",
       "177810             0  \n",
       "94799              1  \n",
       "351134             0  \n",
       "283045             0  "
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now, let's make the same preprocessing as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_data=pd.DataFrame(vpunc(vlower(private_df)))\n",
    "attack_data=pd.DataFrame(vpunc(vlower(attack_df)))\n",
    "private_data.columns=['sentence']\n",
    "attack_data.columns=['sentence']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The next cell takes some times but you can skip it and take the embeddings we have already computed using the cell below this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "private_english_data=private_data.sentence.drop_duplicates()\n",
    "attacker_english_data=attack_data.sentence.drop_duplicates()\n",
    "\n",
    "private_english_input=(private_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "attacker_english_input=(attacker_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "\n",
    "private_english_model = gensim.models.Word2Vec(sentences=private_english_input,min_count=5)\n",
    "attacker_english_model = gensim.models.Word2Vec(sentences=attacker_english_input,min_count=5)\n",
    "\n",
    "private_english_model.save('private_english_model_quora.model')\n",
    "attacker_english_model.save('attacker_english_model_quora.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_english_model=gensim.models.Word2Vec.load('private_english_model_quora.model')\n",
    "attacker_english_model=gensim.models.Word2Vec.load('attacker_english_model_quora.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's search now an example of a sensitive question in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3480</th>\n",
       "      <td>What was the best day of your life so far?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117158</th>\n",
       "      <td>What is the best source of learning astronomy in Pakistan?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11819</th>\n",
       "      <td>Which is the best and genuine immigration consultant in Bangalore?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102596</th>\n",
       "      <td>What is Fiber Optic Transceivers Modules?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50690</th>\n",
       "      <td>Which the best distribution of linux?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145093</th>\n",
       "      <td>How do I think clearly when I am feeling lost and uncertain about the future?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105832</th>\n",
       "      <td>What is the interview process like for a summer internship at Mozilla?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129185</th>\n",
       "      <td>Does DNA change during the life?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144379</th>\n",
       "      <td>Can I slap \"Forever\" stamps on a letter to Canada?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57363</th>\n",
       "      <td>How can you lose weight quickly?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                    0\n",
       "3480                                       What was the best day of your life so far?\n",
       "117158                     What is the best source of learning astronomy in Pakistan?\n",
       "11819              Which is the best and genuine immigration consultant in Bangalore?\n",
       "102596                                      What is Fiber Optic Transceivers Modules?\n",
       "50690                                           Which the best distribution of linux?\n",
       "145093  How do I think clearly when I am feeling lost and uncertain about the future?\n",
       "105832         What is the interview process like for a summer internship at Mozilla?\n",
       "129185                                               Does DNA change during the life?\n",
       "144379                             Can I slap \"Forever\" stamps on a letter to Canada?\n",
       "57363                                                How can you lose weight quickly?"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "private_df=pd.DataFrame(private_df)\n",
    "private_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"How can you lose weight quickly?\" will do the trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " how | confidence: 0.69\n",
      " can | confidence: 0.71\n",
      " you | confidence: 0.67\n",
      " lose | confidence: 0.75\n",
      " belly | confidence: 0.68\n",
      " quickly | confidence: 0.69\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "privacy_leak('How can you lose weight quickly',attacker_english_model,private_english_model)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time, we don't recover the exact same question but the same idea is here and the privacy of the author is at stake. Let's see know the amount of approximately correct guess like before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topn  Accuracy\n",
      "0     1      0.12\n",
      "1     2      0.15\n",
      "2     3      0.17\n",
      "3     4      0.18\n"
     ]
    }
   ],
   "source": [
    "results=pd.DataFrame()\n",
    "accuracies=[]\n",
    "\n",
    "for topn in range(1,5):\n",
    "     accuracies.append(attack_efficiency(attacker_english_model,private_english_model,topn=topn))\n",
    "\n",
    "results['topn']=list(range(1,5))\n",
    "results['Accuracy']=accuracies\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we observe, now we are talking about a rate of 18% of approximately recovered words. This is less than before but still not negligible since as shown above, we can recover sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color:blue;'>  Situation 3 : We consider again that the user is going to make a question on quora.com but this time the attacker does not know that in advance. In that situation, he decides to use a public dataset made of numerous tweets and hope he could find something interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_df=pd.read_csv('quora.csv',encoding='utf-8')\n",
    "\n",
    "twitter_df=pd.read_csv('chat.txt',delimiter='\\t',encoding='utf-8')\n",
    "twitter_df.columns=['sentence']\n",
    "\n",
    "private_df=quora_df.sample(frac=0.7).question1.values\n",
    "attack_df=twitter_df.sample(frac=0.3).sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The next cell takes some times and can lead you to a memory error but you can skip it and take the embeddings we have already computed using the cell below this one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_data=pd.DataFrame(vpunc(vlower(private_df)))\n",
    "attack_data=pd.DataFrame(vpunc(vlower(attack_df)))\n",
    "\n",
    "private_data.columns=['sentence']\n",
    "attack_data.columns=['sentence']\n",
    "\n",
    "private_english_data=private_data.sentence.drop_duplicates()\n",
    "attacker_english_data=attack_data.sentence.drop_duplicates()\n",
    "\n",
    "private_english_input=(private_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "attacker_english_input=(attacker_english_data.apply(lambda x: x.split(\" \")[0:10]).values)\n",
    "\n",
    "private_english_model = gensim.models.Word2Vec(sentences=private_english_input)\n",
    "attacker_english_model = gensim.models.Word2Vec(sentences=attacker_english_input)\n",
    "\n",
    "private_english_model.save('private_english_model_quora_twitter.model')\n",
    "attacker_english_model.save('attacker_english_model_quora_twitter.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_english_model=gensim.models.Word2Vec.load('private_english_model_quora_twitter.model')\n",
    "attacker_english_model=gensim.models.Word2Vec.load('attacker_english_model_quora_twitter.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topn  Accuracy\n",
      "0     1   0.00302\n",
      "1     2   0.00453\n",
      "2     3   0.00613\n",
      "3     4   0.00735\n"
     ]
    }
   ],
   "source": [
    "results=pd.DataFrame()\n",
    "accuracies=[]\n",
    "\n",
    "for topn in range(1,5):\n",
    "    accuracies.append(attack_efficiency(attacker_english_model,private_english_model,topn=topn))\n",
    "\n",
    "results['topn']=list(range(1,5))\n",
    "results['Accuracy']=accuracies\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time the attack seems not really efficient. However nothing tell us that it's not possible to have better results with others word embeddings that are more sophisticated. Moreover, we can still recover private information even with this poor attack as you can see above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i | confidence: 0.41\n",
      " well | confidence: 0.31\n",
      " like | confidence: 0.32\n",
      " trump | confidence: 0.43\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'i well like trump '"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "privacy_leak('I really like trump',attacker_english_model,private_english_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions :\n",
    "\n",
    "- Here, we have shown with very simple experiments that a famous embedding such as Word2Vec is not completely private. Even with two completely different datasets, we succeed in recovering sensitive information. \n",
    "\n",
    "- Adding a laplace noise to the user embedding really helped to make it private. Now, we have to see also to what extent it is an hindrance to the NLP task made by the API. This part is treated in the second illustrative notebook of our study.\n",
    "\n",
    "- In practice if the attacker use a different embedding than the one used for the user dataset, the attack should not lead to satisfying results. Usually, people tend to use famous models that have made their proofs such BERT and so it may be a baseline for an attacker but he can't be totally sure of what to use if the information about the embedding has been private.\n",
    "\n",
    "## Perspectives :\n",
    "\n",
    "- In perspective to this work, we could study the impact of the hidden dimension of the embedding space on the privacy\n",
    "- It would be also interesting to see what's is going one if the user embeddings method is different from the one designed by the attacker as suggested above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
