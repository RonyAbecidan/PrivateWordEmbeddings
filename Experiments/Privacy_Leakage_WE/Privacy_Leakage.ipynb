{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustrative Notebook : Privacy Leakage in NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import warnings\n",
    "from PrivacyLeakWE import *\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situation 1 : We imagine that a part of the dataset used by the user is available online and that the attacker know the entire online dataset\n",
    "\n",
    "#### For this case we have considered a translation dataset from http://www.manythings.org/anki/ mapping english sentences to french sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48972</th>\n",
       "      <td>He used a lot of honey.</td>\n",
       "      <td>Il employait beaucoup de miel.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24544</th>\n",
       "      <td>He is on the radio.</td>\n",
       "      <td>Il passe à la radio.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133538</th>\n",
       "      <td>I'll spend Christmas with my family.</td>\n",
       "      <td>Je passerai Noël avec ma famille.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170923</th>\n",
       "      <td>You don't have to talk about it if you don't w...</td>\n",
       "      <td>Vous n'êtes pas obligés d'en parler si vous ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59823</th>\n",
       "      <td>Tom put on his slippers.</td>\n",
       "      <td>Tom mit ses chaussons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83061</th>\n",
       "      <td>You don't look very strong.</td>\n",
       "      <td>Vous n'avez pas l'air très forts.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80746</th>\n",
       "      <td>That boy displayed no fear.</td>\n",
       "      <td>Ce garçon n'a montré aucune crainte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75136</th>\n",
       "      <td>What are you doing Monday?</td>\n",
       "      <td>Qu'est-ce que vous faites lundi ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119109</th>\n",
       "      <td>Is it cheaper to call after nine?</td>\n",
       "      <td>Est-ce moins cher, d'appeler après neuf heures ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130224</th>\n",
       "      <td>The walls were painted light brown.</td>\n",
       "      <td>Les murs étaient peints de couleur ocre.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  English  \\\n",
       "48972                             He used a lot of honey.   \n",
       "24544                                 He is on the radio.   \n",
       "133538               I'll spend Christmas with my family.   \n",
       "170923  You don't have to talk about it if you don't w...   \n",
       "59823                            Tom put on his slippers.   \n",
       "83061                         You don't look very strong.   \n",
       "80746                         That boy displayed no fear.   \n",
       "75136                          What are you doing Monday?   \n",
       "119109                  Is it cheaper to call after nine?   \n",
       "130224                The walls were painted light brown.   \n",
       "\n",
       "                                                   French  \n",
       "48972                      Il employait beaucoup de miel.  \n",
       "24544                                Il passe à la radio.  \n",
       "133538                  Je passerai Noël avec ma famille.  \n",
       "170923  Vous n'êtes pas obligés d'en parler si vous ne...  \n",
       "59823                              Tom mit ses chaussons.  \n",
       "83061                   Vous n'avez pas l'air très forts.  \n",
       "80746                Ce garçon n'a montré aucune crainte.  \n",
       "75136                   Qu'est-ce que vous faites lundi ?  \n",
       "119109   Est-ce moins cher, d'appeler après neuf heures ?  \n",
       "130224           Les murs étaient peints de couleur ocre.  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df=pd.read_csv('english_french.txt',delimiter='\\t',encoding='utf-8')\n",
    "raw_df.columns=['English','French','Licence']\n",
    "raw_df=raw_df.drop(['Licence'],axis=1)\n",
    "raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape : (179903, 2)\n"
     ]
    }
   ],
   "source": [
    "clean_data=pd.DataFrame(vpunc(vlower(raw_df)))\n",
    "clean_data.columns=raw_df.columns\n",
    "\n",
    "print(f'Dataset shape : {clean_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now the dataset is cleaned, we are going to embed english and french sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "english=clean_data.English.drop_duplicates()\n",
    "private_english_data=english\n",
    "attacker_english_data=(english.sample(frac=0.5))  #a part of the user dataset is public and known by the attacker\n",
    "\n",
    "french=clean_data.French.drop_duplicates()\n",
    "private_french_data=french\n",
    "attacker_french_data=(french.sample(frac=0.5)) #a part of the user dataset is public and known by the attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing consisting in splitting sentences into sequences of words\n",
    "private_english_input=(private_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "attacker_english_input=(attacker_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "\n",
    "private_french_input=(private_french_data.apply(lambda x: x.split(\" \")).values)\n",
    "attacker_french_input=(attacker_french_data.apply(lambda x: x.split(\" \")).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "#private and attacker embeddings for english and french sentences\n",
    "private_english_model = gensim.models.Word2Vec(sentences=private_english_input)\n",
    "attacker_english_model = gensim.models.Word2Vec(sentences=attacker_english_input)\n",
    "\n",
    "private_french_model = gensim.models.Word2Vec(sentences=private_french_input)\n",
    "attacker_french_model = gensim.models.Word2Vec(sentences=attacker_french_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# private_french_model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we can already test if some sensitive sentence can be recovered by the attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('my', 0.728559672832489)]\n",
      "[('doctor', 0.5037205219268799)]\n",
      "[('happened', 0.6517443656921387)]\n",
      "[('that', 0.4703316390514374)]\n",
      "[('i', 0.6370489597320557)]\n",
      "[('only', 0.6262897253036499)]\n",
      "[('have', 0.6257230639457703)]\n",
      "[('a', 0.5301159620285034)]\n",
      "[('few', 0.532633900642395)]\n",
      "[('years', 0.5427626371383667)]\n",
      "[('left', 0.6377806663513184)]\n",
      "[('to', 0.6008734703063965)]\n",
      "[('live', 0.5902172327041626)]\n",
      "\n",
      "\n",
      "[('my', 0.728559672832489)]\n",
      "[('father', 0.5695924758911133)]\n",
      "[('was', 0.6523784399032593)]\n",
      "[('ill', 0.5014663934707642)]\n",
      "[('yesterday', 0.6793800592422485)]\n",
      "\n",
      "\n",
      "[('il', 0.6450693607330322)]\n",
      "[('se', 0.5954228043556213)]\n",
      "[('sens', 0.5272761583328247)]\n",
      "[('seul', 0.5937025547027588)]\n",
      "\n",
      "\n",
      "[('votre', 0.6981475353240967)]\n",
      "[('mot', 0.47707298398017883)]\n",
      "[('de', 0.6065523028373718)]\n",
      "[('passe', 0.47143352031707764)]\n",
      "[('est', 0.6657159328460693)]\n",
      "[('facile', 0.4625858664512634)]\n"
     ]
    }
   ],
   "source": [
    "privacy_leak('my doctor said that I only have a few days left to live',attacker_english_model,private_english_model)\n",
    "print('\\n')\n",
    "privacy_leak('My son was ill yesterday',attacker_english_model,private_english_model)\n",
    "print('\\n')\n",
    "privacy_leak('Il se sent seul',attacker_french_model,private_french_model)\n",
    "print('\\n')\n",
    "privacy_leak('Votre mot de passe est facile' ,attacker_french_model,private_french_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We can also determine the percentage of words recovered by the attacker considering that a word is recovered when he belongs to the k first most probable words proposed by the attacker model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topn</th>\n",
       "      <th>Accuracy_english_dataset</th>\n",
       "      <th>Accuracy_french_dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topn  Accuracy_english_dataset  Accuracy_french_dataset\n",
       "0     1                      0.08                     0.10\n",
       "1     2                      0.11                     0.14\n",
       "2     3                      0.12                     0.16\n",
       "3     4                      0.14                     0.18\n",
       "4     5                      0.15                     0.19"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame()\n",
    "english=[]\n",
    "french=[]\n",
    "\n",
    "for topn in range(1,6):\n",
    "    english.append(attack_efficiency(attacker_english_model,private_english_model,topn=topn))\n",
    "    french.append(attack_efficiency(attacker_french_model,private_french_model,topn=topn))\n",
    "\n",
    "results['topn']=list(range(1,6))\n",
    "results['Accuracy_english_dataset']=english\n",
    "results['Accuracy_french_dataset']=french\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "825.75"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(private_english_model.wv.vocab.keys())*0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the english sentences, we see that we can recover between 10% (550 words) and 15%(826 words) of the private vocabulary. This may seem low but as we have seen before, it is enough for capturing sensitive information !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the embeddings\n",
    "# private_english_model.save('private_english_model_0.5.model')\n",
    "# private_french_model.save('private_french_model_0.5.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#Load our embeddings\n",
    "filename='private_french_model_0.5.model'\n",
    "model = gensim.models.Word2Vec.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situation 2 - The training dataset is entirely private and the attacker has at his disposal a public dataset from the same distribution (for instance sentences of same natures).\n",
    "\n",
    "#### For simulating this situation we will consider the quora pair dataset which is a set of questions pairs that are potentially duplicates in the sense they express the same question without being exactly formulated the same way. We will consider a random sample of the first questions as our private user dataset and a random sample of the second questions as our attacker dataset. We can imagine that the attacker has made himself this dataset collecting questions in quora.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df=pd.read_csv('quora.csv',encoding='utf-8')\n",
    "private_df=raw_df.sample(frac=0.7).question1.values\n",
    "attack_df=raw_df.sample(frac=0.5).question2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_duplicate\n",
       "0    255027\n",
       "1    149263\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.groupby(['is_duplicate']).count()['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we can observe, there are around twice more non duplicates questions than duplicates ones. Hence, our study is reasonable since the dataset are not completely correlated. We can show one case where two questions are completely not duplicates to illustrate that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Why am I mentally very lonely? How can I solve it?']\n",
      "['Find the remainder when [math]23^{24}[/math] is divided by 24,23?']\n"
     ]
    }
   ],
   "source": [
    "print(raw_df[raw_df['id']==3].question1.values)\n",
    "print(raw_df[raw_df['id']==3].question2.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we see, these are two questions completely different. However, there are also a lot of questions that are similar but expressed differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254450</th>\n",
       "      <td>254450</td>\n",
       "      <td>335896</td>\n",
       "      <td>369171</td>\n",
       "      <td>What books were removed from the Bible?</td>\n",
       "      <td>What books are missing from the bible?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245940</th>\n",
       "      <td>245940</td>\n",
       "      <td>358885</td>\n",
       "      <td>358886</td>\n",
       "      <td>What is the full life cycle of an enterprise s...</td>\n",
       "      <td>What is full life cycle?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48460</th>\n",
       "      <td>48460</td>\n",
       "      <td>86397</td>\n",
       "      <td>86398</td>\n",
       "      <td>What does it mean when the dispatch says bith ...</td>\n",
       "      <td>What does it mean when the dispatch says both ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347578</th>\n",
       "      <td>347578</td>\n",
       "      <td>476062</td>\n",
       "      <td>476063</td>\n",
       "      <td>What are some good things to put on the side o...</td>\n",
       "      <td>Which is the best business 4G data only plan f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95299</th>\n",
       "      <td>95299</td>\n",
       "      <td>158972</td>\n",
       "      <td>158973</td>\n",
       "      <td>What kind of Saree will be suitable for my col...</td>\n",
       "      <td>What is the best farewell speech given by a ju...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377690</th>\n",
       "      <td>377690</td>\n",
       "      <td>509022</td>\n",
       "      <td>509023</td>\n",
       "      <td>What is the easiest way to work out fractions?</td>\n",
       "      <td>How do you work out limits of fractions?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375689</th>\n",
       "      <td>375689</td>\n",
       "      <td>506738</td>\n",
       "      <td>506739</td>\n",
       "      <td>How do I play billiards well?</td>\n",
       "      <td>Where and when was billiards first played?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352235</th>\n",
       "      <td>352235</td>\n",
       "      <td>481144</td>\n",
       "      <td>40044</td>\n",
       "      <td>Which is the less harmful cigarette brand?</td>\n",
       "      <td>Which brands of cigarettes sold in the US cont...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181945</th>\n",
       "      <td>181945</td>\n",
       "      <td>278600</td>\n",
       "      <td>278601</td>\n",
       "      <td>What causes facial dimples?</td>\n",
       "      <td>Why do some people have dimples?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60766</th>\n",
       "      <td>60766</td>\n",
       "      <td>106219</td>\n",
       "      <td>106220</td>\n",
       "      <td>I lost my original charger of the OnePlus One....</td>\n",
       "      <td>I have a Moto G 2nd gen. Usually in the day I ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "254450  254450  335896  369171   \n",
       "245940  245940  358885  358886   \n",
       "48460    48460   86397   86398   \n",
       "347578  347578  476062  476063   \n",
       "95299    95299  158972  158973   \n",
       "377690  377690  509022  509023   \n",
       "375689  375689  506738  506739   \n",
       "352235  352235  481144   40044   \n",
       "181945  181945  278600  278601   \n",
       "60766    60766  106219  106220   \n",
       "\n",
       "                                                question1  \\\n",
       "254450            What books were removed from the Bible?   \n",
       "245940  What is the full life cycle of an enterprise s...   \n",
       "48460   What does it mean when the dispatch says bith ...   \n",
       "347578  What are some good things to put on the side o...   \n",
       "95299   What kind of Saree will be suitable for my col...   \n",
       "377690     What is the easiest way to work out fractions?   \n",
       "375689                      How do I play billiards well?   \n",
       "352235         Which is the less harmful cigarette brand?   \n",
       "181945                        What causes facial dimples?   \n",
       "60766   I lost my original charger of the OnePlus One....   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "254450             What books are missing from the bible?             1  \n",
       "245940                           What is full life cycle?             0  \n",
       "48460   What does it mean when the dispatch says both ...             1  \n",
       "347578  Which is the best business 4G data only plan f...             0  \n",
       "95299   What is the best farewell speech given by a ju...             0  \n",
       "377690           How do you work out limits of fractions?             0  \n",
       "375689         Where and when was billiards first played?             0  \n",
       "352235  Which brands of cigarettes sold in the US cont...             0  \n",
       "181945                   Why do some people have dimples?             1  \n",
       "60766   I have a Moto G 2nd gen. Usually in the day I ...             0  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_data=pd.DataFrame(vpunc(vlower(private_df)))\n",
    "attack_data=pd.DataFrame(vpunc(vlower(attack_df)))\n",
    "private_data.columns=['sentence']\n",
    "attack_data.columns=['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "private_english_data=private_data.sentence.drop_duplicates()\n",
    "attacker_english_data=attack_data.sentence.drop_duplicates()\n",
    "\n",
    "private_english_input=(private_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "attacker_english_input=(attacker_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "\n",
    "private_english_model = gensim.models.Word2Vec(sentences=private_english_input)\n",
    "attacker_english_model = gensim.models.Word2Vec(sentences=attacker_english_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 0.7974109649658203)]\n",
      "[('will', 0.7619689106941223)]\n",
      "[('contact', 0.6188812255859375)]\n",
      "[('my', 0.8155895471572876)]\n",
      "[('lawyer', 0.6596115827560425)]\n",
      "[('because', 0.767339825630188)]\n",
      "[('i', 0.7974109649658203)]\n",
      "[('am', 0.8091138005256653)]\n",
      "[('not', 0.6581652164459229)]\n",
      "[('guilty', 0.7783500552177429)]\n",
      "\n",
      "\n",
      "[('i', 0.7974109649658203)]\n",
      "[('have', 0.744394838809967)]\n",
      "[('a', 0.6561230421066284)]\n",
      "[('cancer', 0.7532939910888672)]\n"
     ]
    }
   ],
   "source": [
    "privacy_leak('I will contact my lawyer because I am not guilty',attacker_english_model,private_english_model)\n",
    "print('\\n')\n",
    "privacy_leak('I have a cancer',attacker_english_model,private_english_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topn  Accuracy\n",
      "0     1      0.23\n",
      "1     2      0.27\n",
      "2     3      0.29\n",
      "3     4      0.31\n"
     ]
    }
   ],
   "source": [
    "results=pd.DataFrame()\n",
    "accuracies=[]\n",
    "\n",
    "for topn in range(1,5):\n",
    "     accuracies.append(attack_efficiency(attacker_english_model,private_english_model,topn=topn))\n",
    "\n",
    "results['topn']=list(range(1,5))\n",
    "results['Accuracy']=accuracies\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3479.2000000000003"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(private_english_model.wv.vocab.keys())*0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As we observe, this time we are talking about 20% of recovery of the dataset (which corresponds approximately here  to 3479 words). This is not negligible and as shown above, we can recover very sensitive information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Situation 3 : We consider completely different datasets but that could potentially share some information. For instance, we consider again the quora dataset made of questions asked by some users and an other dataset that an attacker will use which is made of tweets published by some users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "quora_df=pd.read_csv('quora.csv',encoding='utf-8')\n",
    "twitter_df=pd.read_csv('chat.txt',delimiter='\\t',encoding='utf-8')\n",
    "twitter_df.columns=['sentence']\n",
    "# quora_df=quora_df.sample(frac=0.015)\n",
    "private_df=quora_df.question1.values\n",
    "attack_df=twitter_df.sample(frac=0.2).sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_data=pd.DataFrame(vpunc(vlower(private_df)))\n",
    "attack_data=pd.DataFrame(vpunc(vlower(attack_df)))\n",
    "private_data.columns=['sentence']\n",
    "attack_data.columns=['sentence']\n",
    "np.random.seed(0)\n",
    "\n",
    "private_english_data=private_data.sentence.drop_duplicates()\n",
    "attacker_english_data=attack_data.sentence.drop_duplicates()\n",
    "\n",
    "private_english_input=(private_english_data.apply(lambda x: x.split(\" \")).values)\n",
    "attacker_english_input=(attacker_english_data.apply(lambda x: x.split(\" \")[0:10]).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_english_model = gensim.models.Word2Vec(sentences=private_english_input)\n",
    "attacker_english_model = gensim.models.Word2Vec(sentences=attacker_english_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_efficiency(attack_model,private_model,topn=3,display=False):\n",
    "    private_words=private_model.wv.vocab.keys()\n",
    "    s=0\n",
    "    for word in private_words:\n",
    "        l=[]\n",
    "        most_similar_words=attack_model.similar_by_vector(private_model[word], topn=topn, restrict_vocab=None)\n",
    "        for prop,_ in most_similar_words:\n",
    "            l.append(prop)\n",
    "            \n",
    "        s+= int(word in l)\n",
    "        if display:\n",
    "            if(word in l):\n",
    "                print(word)\n",
    "        \n",
    "    return round(s/len(private_words),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   topn  Accuracy\n",
      "0     1   0.00273\n",
      "1     2   0.00433\n",
      "2     3   0.00560\n",
      "3     4   0.00633\n"
     ]
    }
   ],
   "source": [
    "results=pd.DataFrame()\n",
    "accuracies=[]\n",
    "\n",
    "for topn in range(1,5):\n",
    "    accuracies.append(attack_efficiency(attacker_english_model,private_english_model,topn=topn))\n",
    "\n",
    "results['topn']=list(range(1,5))\n",
    "results['Accuracy']=accuracies\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This time the attack is not really efficient. However nothing tell us that it's not possible to have better results with others word embeddings that are more sophisticated. Moreover, we can still recover private information even with this poor attack as you can see above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 0.34840452671051025)]\n",
      "[('like', 0.39880162477493286)]\n",
      "[('trump', 0.4790140688419342)]\n"
     ]
    }
   ],
   "source": [
    "privacy_leak('I like trump',attacker_english_model,private_english_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
